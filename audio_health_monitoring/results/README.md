# Results Folder

This folder contains all training, evaluation, and inference artifacts generated by the audio health monitoring models.  
It is split by model type for clarity and reproducibility.

---

## üìÅ cnn_results/

Artifacts produced by the **CNN (log-mel spectrogram) model**.

Typical contents:
- **`cnn_test_inference_*.csv`**  
  Parent-level predictions on the held-out test set (mean of segment scores).

- **`cnn_spec_cache.npz`**  
  Cached log-mel spectrograms for all audio segments to avoid recomputation.

- **`split_manifest.json`**  
  Exact train / validation / test split used during CNN training (prevents data leakage).

- **Training plots** (`*_loss.png`, `*_accuracy.png`)  
  CNN learning curves over epochs.

- **Confusion matrix / ROC / PR curves**  
  Visual evaluation of CNN performance on the test set.

---

## üìÅ yamnet_results/

Artifacts produced by the **YAMNet + BiLSTM sequence model**.

Typical contents:
- **`metrics_summary_v2_*.json`**  
  Primary experiment summary including:
  - Best validation threshold  
  - Test ROC-AUC & Average Precision  
  - Paths to saved models

- **`classification_report_v2_*.csv`**  
  Precision, recall, F1-score, and support for each class.

- **`test_predictions_v2_*.csv`**  
  Parent-level predictions on the test split.

- **Embedding cache** (`segment_embeddings_cache_v2.npz`)  
  Cached YAMNet embeddings for fast retraining.

- **Plots**  
  ROC curve, PR curve, confusion matrix, and training history.

---

## üß™ Naming Convention

All result files include a timestamp:
```
<artifact_name>_YYYYMMDD_HHMMSS.*
```


This guarantees full experiment traceability and prevents accidental overwrites.

---

## ‚úÖ Notes

- All evaluations are **parent-level**, not segment-level.
- Test splits are fixed and reproducible.
- Metrics reported here are the **final authoritative results** for the project.
